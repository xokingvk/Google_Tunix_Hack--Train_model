{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14446110,"sourceType":"datasetVersion","datasetId":8870203}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install transformers accelerate peft bitsandbytes trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# Load the JSONL file into a pandas DataFrame\ndf = pd.read_json('/kaggle/input/pytrainclean-jsonl-datasetreasoning-finalcode/train.jsonl', lines=True)\n\n# Convert the pandas DataFrame to a Hugging Face Dataset object\nhf_dataset = Dataset.from_pandas(df)\n\nprint(\"Dataset loaded successfully as pandas DataFrame and converted to Hugging Face Dataset.\")\nprint(hf_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# 1. Load the pre-trained tokenizer for 'google/gemma-2-2b'\ntokenizer = AutoTokenizer.from_pretrained('google/gemma-2-2b')\n\n# Add a pad token if it doesn't exist, which is common for some models like Gemma\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'}) # Or use a token that is not used in your vocabulary.\n\n# 2. Define a function to format each example\ndef format_example(example):\n    # Concatenate 'question', 'steps', and 'final_code' into a single string\n    # This assumes a simple prompt-response structure for fine-tuning.\n    # You might want to adjust this based on the specific fine-tuning method (e.g., SFTTrainer)\n    # For simplicity, let's create a conversational turn format.\n    formatted_text = f\"User: {example['question']}\\nAssistant: {example['steps']}\\n{example['final_code']}\"\n    return {'text': formatted_text}\n\n# 3. Apply the formatting function to the hf_dataset\nhf_dataset_formatted = hf_dataset.map(format_example, batched=False)\n\n# 4. Tokenize the formatted dataset\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n\ntokenized_dataset = hf_dataset_formatted.map(tokenize_function, batched=True)\n\nprint(\"Dataset formatted and tokenized successfully.\")\nprint(tokenized_dataset)\nprint(\"Sample of tokenized data:\")\nprint(tokenized_dataset[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\n# 1. Define the model ID\nmodel_id = 'google/gemma-2-2b'\n\n# 2. Configure 4-bit quantization\nbitsandbytes_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n)\n\n# 3. Load the pre-trained Gemma 2B model with quantization and device_map\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bitsandbytes_config,\n    device_map=\"auto\",\n)\n\n# 4. Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# 5. Add a pad token if it doesn't exist\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Or use a token that is not used in your vocabulary.\n\nprint(f\"Model '{model_id}' loaded successfully with 4-bit quantization.\")\nprint(f\"Tokenizer for '{model_id}' loaded successfully.\")\nprint(f\"Pad token set: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import LoraConfig\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    bias='none',\n    task_type='CAUSAL_LM',\n)\n\nprint(\"LoRA configuration created successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom trl import SFTConfig,SFTTrainer\n\n# 1. Define Training Arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    optim='paged_adamw_8bit',\n    save_steps=100,\n    logging_steps=10,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type='constant',\n    report_to='tensorboard',\n    remove_unused_columns=False\n)\n\n# Training arguments\ntraining_args = SFTConfig(  \n        output_dir=\"mistral-finetuned-alpaca\",\n        per_device_train_batch_size=8,\n        gradient_accumulation_steps=1,\n        optim=\"paged_adamw_32bit\",\n        learning_rate=2e-4,\n        lr_scheduler_type=\"cosine\",\n        save_strategy=\"epoch\",\n        logging_steps=100,\n        num_train_epochs=1,\n        max_steps=250,\n        fp16=True,\n        packing=False,  \n\n        dataset_text_field=\"text\",  \n        push_to_hub=True\n)\n\n# Initialize Trainer\ntrainer = SFTTrainer(\n        model=model,\n        train_dataset=tokenized_dataset,\n        peft_config=lora_config,\n        args=training_args,\n)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Starting model training...')\ntrainer.train()\n\n# Save the fine-tuned model\noutput_dir = './fine_tuned_model'\ntrainer.save_model(output_dir)\nprint(f\"Fine-tuned model saved to {output_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}